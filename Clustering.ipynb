{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "1 ['i']\n",
      "2 []\n",
      "1 ['it']\n",
      "2 []\n",
      "1 ['d']\n",
      "2 []\n",
      "1 ['own']\n",
      "2 []\n",
      "1 ['while']\n",
      "2 []\n",
      "1 ['each']\n",
      "2 []\n",
      "1 ['you', \"'re\"]\n",
      "2 [\"'re\"]\n",
      "1 ['have', \"n't\"]\n",
      "2 [\"n't\"]\n",
      "1 ['ain']\n",
      "2 []\n",
      "1 ['o']\n",
      "2 []\n",
      "1 ['they']\n",
      "2 []\n",
      "1 ['again']\n",
      "2 []\n",
      "1 ['an']\n",
      "2 []\n",
      "1 ['you', \"'ve\"]\n",
      "2 [\"'ve\"]\n",
      "1 ['with']\n",
      "2 []\n",
      "1 ['do', \"n't\"]\n",
      "2 [\"n't\"]\n",
      "1 ['does', \"n't\"]\n",
      "2 [\"n't\"]\n",
      "1 ['wasn']\n",
      "2 []\n",
      "1 ['below']\n",
      "2 []\n",
      "1 ['under']\n",
      "2 []\n",
      "1 ['yourself']\n",
      "2 []\n",
      "1 ['for']\n",
      "2 []\n",
      "1 ['whom']\n",
      "2 []\n",
      "1 ['might', \"n't\"]\n",
      "2 ['might', \"n't\"]\n",
      "1 ['doesn']\n",
      "2 []\n",
      "1 ['is', \"n't\"]\n",
      "2 [\"n't\"]\n",
      "1 ['against']\n",
      "2 []\n",
      "1 ['my']\n",
      "2 []\n",
      "1 ['this']\n",
      "2 []\n",
      "1 ['sha', \"n't\"]\n",
      "2 ['sha', \"n't\"]\n",
      "1 ['weren']\n",
      "2 []\n",
      "1 ['both']\n",
      "2 []\n",
      "1 ['need', \"n't\"]\n",
      "2 ['need', \"n't\"]\n",
      "1 ['mustn']\n",
      "2 []\n",
      "1 ['some']\n",
      "2 []\n",
      "1 ['ma']\n",
      "2 []\n",
      "1 ['themselves']\n",
      "2 []\n",
      "1 ['wouldn']\n",
      "2 []\n",
      "1 ['doing']\n",
      "2 []\n",
      "1 ['has']\n",
      "2 []\n",
      "1 ['only']\n",
      "2 []\n",
      "1 ['isn']\n",
      "2 []\n",
      "1 ['didn']\n",
      "2 []\n",
      "1 ['wo', \"n't\"]\n",
      "2 ['wo', \"n't\"]\n",
      "1 ['were']\n",
      "2 []\n",
      "1 ['her']\n",
      "2 []\n",
      "1 ['above']\n",
      "2 []\n",
      "1 ['is']\n",
      "2 []\n",
      "1 ['there']\n",
      "2 []\n",
      "1 ['not']\n",
      "2 []\n",
      "1 ['were', \"n't\"]\n",
      "2 [\"n't\"]\n",
      "1 ['did', \"n't\"]\n",
      "2 [\"n't\"]\n",
      "1 ['mightn']\n",
      "2 []\n",
      "1 ['so']\n",
      "2 []\n",
      "1 ['being']\n",
      "2 []\n",
      "1 ['why']\n",
      "2 []\n",
      "1 ['ourselves']\n",
      "2 []\n",
      "1 ['them']\n",
      "2 []\n",
      "1 ['its']\n",
      "2 []\n",
      "1 ['don']\n",
      "2 []\n",
      "1 ['should', \"n't\"]\n",
      "2 [\"n't\"]\n",
      "1 ['herself']\n",
      "2 []\n",
      "1 ['in']\n",
      "2 []\n",
      "1 ['that', \"'ll\"]\n",
      "2 [\"'ll\"]\n",
      "1 ['does']\n",
      "2 []\n",
      "1 ['then']\n",
      "2 []\n",
      "1 ['their']\n",
      "2 []\n",
      "1 ['from']\n",
      "2 []\n",
      "1 ['should', \"'ve\"]\n",
      "2 [\"'ve\"]\n",
      "1 ['who']\n",
      "2 []\n",
      "1 ['have']\n",
      "2 []\n",
      "1 ['more']\n",
      "2 []\n",
      "1 ['aren']\n",
      "2 []\n",
      "1 ['couldn']\n",
      "2 []\n",
      "1 ['ve']\n",
      "2 []\n",
      "1 ['during']\n",
      "2 []\n",
      "1 ['and']\n",
      "2 []\n",
      "1 ['or']\n",
      "2 []\n",
      "1 ['down']\n",
      "2 []\n",
      "1 ['just']\n",
      "2 []\n",
      "1 ['did']\n",
      "2 []\n",
      "1 ['here']\n",
      "2 []\n",
      "1 ['because']\n",
      "2 []\n",
      "1 ['to']\n",
      "2 []\n",
      "1 ['do']\n",
      "2 []\n",
      "1 ['hers']\n",
      "2 []\n",
      "1 ['had', \"n't\"]\n",
      "2 [\"n't\"]\n",
      "1 ['could', \"n't\"]\n",
      "2 ['could', \"n't\"]\n",
      "1 ['theirs']\n",
      "2 []\n",
      "1 ['how']\n",
      "2 []\n",
      "1 ['can']\n",
      "2 []\n",
      "1 ['needn']\n",
      "2 []\n",
      "1 ['you']\n",
      "2 []\n",
      "1 ['between']\n",
      "2 []\n",
      "1 ['ll']\n",
      "2 []\n",
      "1 ['out']\n",
      "2 []\n",
      "1 ['on']\n",
      "2 []\n",
      "1 ['than']\n",
      "2 []\n",
      "1 ['shan']\n",
      "2 []\n",
      "1 ['that']\n",
      "2 []\n",
      "1 ['too']\n",
      "2 []\n",
      "1 ['would', \"n't\"]\n",
      "2 ['would', \"n't\"]\n",
      "1 ['yourselves']\n",
      "2 []\n",
      "1 ['y']\n",
      "2 []\n",
      "1 ['had']\n",
      "2 []\n",
      "1 ['me']\n",
      "2 []\n",
      "1 ['what']\n",
      "2 []\n",
      "1 ['nor']\n",
      "2 []\n",
      "1 ['are']\n",
      "2 []\n",
      "1 ['should']\n",
      "2 []\n",
      "1 ['it', \"'s\"]\n",
      "2 [\"'s\"]\n",
      "1 ['about']\n",
      "2 []\n",
      "1 ['himself']\n",
      "2 []\n",
      "1 ['all']\n",
      "2 []\n",
      "1 ['up']\n",
      "2 []\n",
      "1 ['where']\n",
      "2 []\n",
      "1 ['won']\n",
      "2 []\n",
      "1 ['same']\n",
      "2 []\n",
      "1 ['s']\n",
      "2 []\n",
      "1 ['now']\n",
      "2 []\n",
      "1 ['haven']\n",
      "2 []\n",
      "1 ['must', \"n't\"]\n",
      "2 ['must', \"n't\"]\n",
      "1 ['our']\n",
      "2 []\n",
      "1 ['we']\n",
      "2 []\n",
      "1 ['she']\n",
      "2 []\n",
      "1 ['having']\n",
      "2 []\n",
      "1 ['was', \"n't\"]\n",
      "2 [\"n't\"]\n",
      "1 ['which']\n",
      "2 []\n",
      "1 ['he']\n",
      "2 []\n",
      "1 ['the']\n",
      "2 []\n",
      "1 ['such']\n",
      "2 []\n",
      "1 ['any']\n",
      "2 []\n",
      "1 ['itself']\n",
      "2 []\n",
      "1 ['off']\n",
      "2 []\n",
      "1 ['over']\n",
      "2 []\n",
      "1 ['through']\n",
      "2 []\n",
      "1 ['hadn']\n",
      "2 []\n",
      "1 ['few']\n",
      "2 []\n",
      "1 ['but']\n",
      "2 []\n",
      "1 ['very']\n",
      "2 []\n",
      "1 ['yours']\n",
      "2 []\n",
      "1 ['be']\n",
      "2 []\n",
      "1 ['until']\n",
      "2 []\n",
      "1 ['into']\n",
      "2 []\n",
      "1 ['after']\n",
      "2 []\n",
      "1 ['m']\n",
      "2 []\n",
      "1 ['re']\n",
      "2 []\n",
      "1 ['of']\n",
      "2 []\n",
      "1 ['him']\n",
      "2 []\n",
      "1 ['no']\n",
      "2 []\n",
      "1 ['am']\n",
      "2 []\n",
      "1 ['you', \"'d\"]\n",
      "2 [\"'d\"]\n",
      "1 ['t']\n",
      "2 []\n",
      "1 ['if']\n",
      "2 []\n",
      "1 ['these']\n",
      "2 []\n",
      "1 ['before']\n",
      "2 []\n",
      "1 ['most']\n",
      "2 []\n",
      "1 ['has', \"n't\"]\n",
      "2 [\"n't\"]\n",
      "1 ['she', \"'s\"]\n",
      "2 [\"'s\"]\n",
      "1 ['you', \"'ll\"]\n",
      "2 [\"'ll\"]\n",
      "1 ['further']\n",
      "2 []\n",
      "1 ['are', \"n't\"]\n",
      "2 [\"n't\"]\n",
      "1 ['a']\n",
      "2 []\n",
      "1 ['other']\n",
      "2 []\n",
      "1 ['myself']\n",
      "2 []\n",
      "1 ['once']\n",
      "2 []\n",
      "1 ['was']\n",
      "2 []\n",
      "1 ['your']\n",
      "2 []\n",
      "1 ['hasn']\n",
      "2 []\n",
      "1 ['ours']\n",
      "2 []\n",
      "1 ['will']\n",
      "2 []\n",
      "1 ['at']\n",
      "2 []\n",
      "1 ['those']\n",
      "2 []\n",
      "1 ['when']\n",
      "2 []\n",
      "1 ['by']\n",
      "2 []\n",
      "1 ['been']\n",
      "2 []\n",
      "1 ['as']\n",
      "2 []\n",
      "1 ['shouldn']\n",
      "2 []\n",
      "1 ['his']\n",
      "2 []\n",
      "1 ['name', 'a', 'few', 'important', 'dates', 'during', 'the', 'world', 'war', '2']\n",
      "2 ['name', 'import', 'date', 'world', 'war', '2']\n",
      "1 ['write', 'a', 'short', 'note', 'on', 'cvivl', 'war']\n",
      "2 ['write', 'short', 'note', 'cvivl', 'war']\n",
      "1 ['what', 'are', 'subatomic', 'particles', '?']\n",
      "2 ['subatom', 'particl', '?']\n",
      "1 ['does', 'dust', 'particles', 'cause', 'itching', 'as', 'the', 'eyes', '?']\n",
      "2 ['dust', 'particl', 'caus', 'itch', 'eye', '?']\n",
      "1 ['explain', 'seven', 'years', 'itch', 'as', 'a', 'movie']\n",
      "2 ['explain', 'seven', 'year', 'itch', 'movi']\n",
      "  (0, 11)\t0.4206690600631704\n",
      "  (0, 8)\t0.4206690600631704\n",
      "  (0, 4)\t0.4206690600631704\n",
      "  (0, 18)\t0.4206690600631704\n",
      "  (0, 17)\t0.3393931489111758\n",
      "  (0, 0)\t0.4206690600631704\n",
      "  (1, 17)\t0.3741047724501572\n",
      "  (1, 19)\t0.4636932227319092\n",
      "  (1, 15)\t0.4636932227319092\n",
      "  (1, 12)\t0.4636932227319092\n",
      "  (1, 3)\t0.4636932227319092\n",
      "  (2, 16)\t0.6591180018251055\n",
      "  (2, 13)\t0.5317722537280788\n",
      "  (2, 1)\t0.5317722537280788\n",
      "  (3, 13)\t0.36252617931707154\n",
      "  (3, 1)\t0.36252617931707154\n",
      "  (3, 5)\t0.4493418549869352\n",
      "  (3, 2)\t0.4493418549869352\n",
      "  (3, 9)\t0.36252617931707154\n",
      "  (3, 7)\t0.4493418549869352\n",
      "  (4, 9)\t0.3741047724501572\n",
      "  (4, 6)\t0.4636932227319092\n",
      "  (4, 14)\t0.4636932227319092\n",
      "  (4, 20)\t0.4636932227319092\n",
      "  (4, 10)\t0.4636932227319092\n",
      "['Name a few important dates during the World WAr 2']\n",
      "[' Write a short note on cvivl war']\n",
      "[' what are subatomic particles?', 'does dust particles cause itching as the eyes?']\n",
      "['Explain seven years itch as a movie']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meghana/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def word_tokenizer(text):\n",
    "            #tokenizes and stems the text\n",
    "            tokens = word_tokenize(text)\n",
    "            stemmer = PorterStemmer()\n",
    "            print(\"1\",tokens)\n",
    "            tokens = [stemmer.stem(t) for t in tokens if t not in stopwords.words('english')]\n",
    "            print(\"2\",tokens)\n",
    "            return tokens\n",
    "\n",
    "\n",
    "def cluster_sentences_tfidf(sentences, nb_of_clusters=5):\n",
    "        tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenizer,\n",
    "                                        stop_words=stopwords.words('english'),\n",
    "                                        max_df=0.9,\n",
    "                                        min_df=0.1,\n",
    "                                        lowercase=True)\n",
    "        #builds a tf-idf matrix for the sentences\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "        print(tfidf_matrix)\n",
    "        kmeans = KMeans(n_clusters=nb_of_clusters)\n",
    "        kmeans.fit(tfidf_matrix)\n",
    "        clusters = collections.defaultdict(list)\n",
    "        for i, label in enumerate(kmeans.labels_):\n",
    "                clusters[label].append(i)\n",
    "        return dict(clusters)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Hello\")\n",
    "    dataset = ['Name a few important dates during the World WAr 2',' Write a short note on cvivl war',' what are subatomic particles?','does dust particles cause itching as the eyes?','Explain seven years itch as a movie']\n",
    "    b = cluster_sentences_tfidf(dataset,4)\n",
    "    for i in b:\n",
    "        print([dataset[h] for h in b[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My name is Meghana',' My nmae is Sanjana',' Today is a good day',' What a pleasnt day!',' It is very sunny outside\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"My name is Meghana. My nmae is Sanjana. Today is a good day. What a pleasnt day!. It is very sunny outside\".replace(\".\",\"\\',\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nvectorizer = TfidfTransformer()\\nX = vectorizer.fit_transform(dataset.data)\\n\\nsvd = TruncatedSVD(true_k)\\nlsa = make_pipeline(svd, Normalizer(copy=False))\\n\\nX = lsa.fit_transform(X)\\n\\nkm = KMeans(n_clusters=true_k, init='k-means++', max_iter=100)\\nkm.fit(X)\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "vectorizer = TfidfTransformer()\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "svd = TruncatedSVD(true_k)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100)\n",
    "km.fit(X)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
